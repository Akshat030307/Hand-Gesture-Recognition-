import cv2
import mediapipe as mp
import numpy as np
from pycaw.pycaw import AudioUtilities, IAudioEndpointVolume
from comtypes import CLSCTX_ALL
import math
import time


mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(min_detection_confidence=0.7, min_tracking_confidence=0.7)


devices = AudioUtilities.GetSpeakers()
interface = devices.Activate(IAudioEndpointVolume._iid_, CLSCTX_ALL, None)
volume = interface.QueryInterface(IAudioEndpointVolume)

vol_range = volume.GetVolumeRange()
min_vol = vol_range[0]
max_vol = vol_range[1]


last_action_time = time.time()
action_delay = 0.5 

locked_volume = None


def recognize_gesture(landmarks):
    if landmarks:
        thumb_tip = landmarks[mp_hands.HandLandmark.THUMB_TIP].y
        thumb_mcp = landmarks[mp_hands.HandLandmark.THUMB_MCP].y
        index_tip = landmarks[mp_hands.HandLandmark.INDEX_FINGER_TIP].y
        middle_tip = landmarks[mp_hands.HandLandmark.MIDDLE_FINGER_TIP].y
        ring_tip = landmarks[mp_hands.HandLandmark.RING_FINGER_TIP].y
        pinky_tip = landmarks[mp_hands.HandLandmark.PINKY_TIP].y

       
        if thumb_tip < thumb_mcp and thumb_tip < index_tip:
            return "Thumbs Up"

 
        if thumb_tip > thumb_mcp and thumb_tip > index_tip:
            return "Thumbs Down"

  
        if (thumb_tip > thumb_mcp and index_tip > middle_tip and 
            middle_tip > ring_tip and ring_tip > pinky_tip):
            return "Close"

    return "Unknown Gesture"

def calculate_distance(point1, point2):
    return math.sqrt((point1.x - point2.x)**2 + (point1.y - point2.y)**2 + (point1.z - point2.z)**2)

# OpenCV Video Capture
cap = cv2.VideoCapture(0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        print("Ignoring empty camera frame.")
        continue


    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(frame_rgb)

  
    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)
            
          
            landmarks = hand_landmarks.landmark
            
          
            gesture = recognize_gesture(landmarks)
            cv2.putText(frame, gesture, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2, cv2.LINE_AA)

          
            current_time = time.time()
            if current_time - last_action_time > action_delay:
                if gesture != "Close":
                   
                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]
                    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]
                    distance = calculate_distance(thumb_tip, index_tip)

                  
                    normalized_vol = np.interp(distance, [0.02, 0.2], [min_vol, max_vol])

                    if locked_volume is None:
                        volume.SetMasterVolumeLevel(normalized_vol, None)

    
                    last_action_time = current_time

                    vol_percent = np.interp(normalized_vol, [min_vol, max_vol], [0, 100])
                    cv2.putText(frame, f"Volume: {int(vol_percent)}%", (10, 100), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)

                elif gesture == "Close":
                    if locked_volume is None:
                        locked_volume = volume.GetMasterVolumeLevel()
                        print(f"Volume locked at {locked_volume}dB")
                    cv2.putText(frame, "Volume Locked", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)
            if gesture != "Close":
                locked_volume = None

    cv2.imshow("Gesture Recognition", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
